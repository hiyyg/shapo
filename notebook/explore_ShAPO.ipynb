{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fd0ad9",
   "metadata": {},
   "source": [
    "# Explore Inference and Optimization of ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c307d51",
   "metadata": {},
   "source": [
    "This is a colab to explore ShAPO inference and optimization properties, proposed in our work [ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization](https://zubair-irshad.github.io/projects/ShAPO.html).\n",
    "#### Make sure that you have enabled the GPU under Runtime-> Change runtime type!\n",
    "\n",
    "\n",
    "We will then reproduce the following results from the paper:\n",
    "\n",
    "1. [**Single Shot inference**](#Single-Shot-inference)\n",
    "\n",
    "    1.1 [Visualize peak and depth output](#Visualize-Peaks-and-Depth-output)\n",
    "    \n",
    "    1.2 [Decode shape with predicted textures from shape and appearance embeddings](#Decode-shape-with-predicted-textures-from-shape-and-appearance-embeddings)\n",
    "    \n",
    "    1.3 [Project 3D Pointclouds and 3D bounding boxes on 2D image](#Project-3D-Pointclouds-and-3D-bounding-boxes-on-2D-image)\n",
    "    \n",
    "    \n",
    "2. [**Shape, Appearance and Pose Optimization**](#Shape-Appearance-and-Pose-Optimization)\n",
    "\n",
    "    2.1 [Core optimization loop](Core-optimization-loop)\n",
    "    \n",
    "    2.2 [Viusalizing optimized 3D output](Viusalizing-optimized-3D-output)\n",
    "\n",
    "Let's get started! The whole notebook takes ~5 minutes or so to run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab91ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this on Google Colab\n",
    "!git clone https://github.com/zubair-irshad/shapo.git\n",
    "!pip install --upgrade pip\n",
    "!cd shapo && pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!cd shapo && wget https://www.dropbox.com/s/cvqyhr67zpxyq36/test_subset.tar.xz?dl=1 -O test_subset.tar.xz && tar -xvf test_subset.tar.xz\n",
    "!cd shapo && wget https://www.dropbox.com/s/929kz7zuxw8jajy/sdf_rgb_pretrained.tar.xz?dl=1 -O sdf_rgb_pretrained.tar.xz && tar -xvf sdf_rgb_pretrained.tar.xz\n",
    "!cd shapo && wget https://www.dropbox.com/s/nrsl67ir6fml9ro/ckpts.tar.xz?dl=1 -O ckpts.tar.xz && tar -xvf ckpts.tar.xz\n",
    "!cd shapo && mkdir test_data && mv test_subset/* test_data && mv sdf_rgb_pretrained test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a6bfe",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93972c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import _pickle as cPickle\n",
    "import os, sys\n",
    "sys.path.append('shapo')\n",
    "from simnet.lib.net import common\n",
    "from simnet.lib import camera\n",
    "from simnet.lib.net.panoptic_trainer import PanopticModel\n",
    "from utils.nocs_utils import load_img_NOCS, create_input_norm\n",
    "from utils.viz_utils import depth2inv, viz_inv_depth\n",
    "from utils.transform_utils import get_gt_pointclouds, transform_coordinates_3d, calculate_2d_projections\n",
    "from utils.transform_utils import project, get_pc_absposes, transform_pcd_to_canonical\n",
    "from utils.viz_utils import save_projected_points, draw_bboxes, line_set_mesh, display_gird, draw_geometries, show_projected_points\n",
    "from sdf_latent_codes.get_surface_pointcloud import get_surface_pointclouds_octgrid_viz, get_surface_pointclouds\n",
    "from sdf_latent_codes.get_rgb import get_rgbnet, get_rgb_from_rgbnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbd124",
   "metadata": {},
   "source": [
    "# ShAPO Model (Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956ded1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.argv = ['', '@shapo/configs/net_config.txt']\n",
    "parser = argparse.ArgumentParser(fromfile_prefix_chars='@')\n",
    "common.add_train_args(parser)\n",
    "app_group = parser.add_argument_group('app')\n",
    "app_group.add_argument('--app_output', default='inference', type=str)\n",
    "app_group.add_argument('--result_name', default='shapo_inference', type=str)\n",
    "app_group.add_argument('--data_dir', default='shapo/test_data', type=str)\n",
    "\n",
    "hparams = parser.parse_args()\n",
    "min_confidence = 0.50\n",
    "use_gpu=True\n",
    "hparams.checkpoint = 'shapo/ckpts/shapo_real.ckpt'\n",
    "model = PanopticModel(hparams, 0, None, None)\n",
    "model.eval()\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "data_path = open(os.path.join(hparams.data_dir, 'Real', 'test_list_subset.txt')).read().splitlines()\n",
    "_CAMERA = camera.NOCS_Real()\n",
    "sdf_pretrained_dir = os.path.join(hparams.data_dir, 'sdf_rgb_pretrained')\n",
    "rgb_model_dir = os.path.join(hparams.data_dir, 'sdf_rgb_pretrained', 'rgb_net_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9563a",
   "metadata": {},
   "source": [
    "# Single Shot inference\n",
    " Note that how this part is similar to [CenterSnap](https://zubair-irshad.github.io/projects/CenterSnap.html) and we predict *SDF embeddings* instead of *pointcloud embeddings*. We further predict *appearance embeddings* and *segmentation masks* as well for downstream optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num from 0 to 3 (small subset of data)\n",
    "num = 0\n",
    "img_full_path = os.path.join(hparams.data_dir, 'Real', data_path[num])\n",
    "img_vis = cv2.imread(img_full_path + '_color.png')\n",
    "\n",
    "left_linear, depth, actual_depth = load_img_NOCS(img_full_path + '_color.png' , img_full_path + '_depth.png')\n",
    "input = create_input_norm(left_linear, depth)[None, :, :, :]\n",
    "    \n",
    "if use_gpu:\n",
    "    input = input.to(torch.device('cuda:0'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_output, _, _ , pose_output = model.forward(input)\n",
    "    _, _, _ , pose_output = model.forward(input)\n",
    "    shape_emb_outputs, appearance_emb_outputs, abs_pose_outputs, peak_output, scores_out, output_indices = pose_output.compute_shape_pose_and_appearance(min_confidence,is_target = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077e606",
   "metadata": {},
   "source": [
    "### Visualize Peaks and Depth output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gird(img_vis, depth, peak_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3188d",
   "metadata": {},
   "source": [
    "## Decode shape with predicted textures from shape and appearance embeddings\n",
    "\n",
    "\n",
    "\n",
    "**Note:** The expected output here is colored pointclouds. Although our shape representation is implicit (i.e. SDF), we only output pointclouds here for computational reasons (i.e. marching cubes output would take some time). If you are interested in getting a mesh, please see save_mesh function in `save_canonical_mesh.py`.\n",
    "\n",
    "`Click on orbital rotation on the top right side to move the colored pointclouds smoothly.`\n",
    "\n",
    "**Note:** The shape and pose predictions are really good from the single-shot prediction whereas you'll see appearance embeddings doesn't seem to be there yet. Hence we will perform optimization giving a single-view RGB-D. Please see **4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_pcds = []\n",
    "points_2d = []\n",
    "box_obb = []\n",
    "axes = [] \n",
    "lod = 7 # Choose from LOD 3-7 here, going higher means more memory and finer details\n",
    "\n",
    "# Here we visualize the output of our network\n",
    "for j in range(len(shape_emb_outputs)):\n",
    "    shape_emb = shape_emb_outputs[j]\n",
    "    # appearance_emb = appearance_emb_putputs[j]\n",
    "    appearance_emb = appearance_emb_outputs[j]\n",
    "    is_oct_grid = True\n",
    "    if is_oct_grid:\n",
    "        # pcd_dsdf_actual = get_surface_pointclouds_octgrid_sparse(shape_emb, sdf_latent_code_dir = sdf_pretrained_dir, lods=[2,3,4,5,6])\n",
    "        pcd_dsdf, nrm_dsdf = get_surface_pointclouds_octgrid_viz(shape_emb, lod=lod, sdf_latent_code_dir=sdf_pretrained_dir)\n",
    "    else:\n",
    "        pcd_dsdf = get_surface_pointclouds(shape_emb)\n",
    "    rgbnet = get_rgbnet(rgb_model_dir)\n",
    "    pred_rgb = get_rgb_from_rgbnet(shape_emb, pcd_dsdf, appearance_emb, rgbnet)\n",
    "    rotated_pc, rotated_box, _ = get_pc_absposes(abs_pose_outputs[j], pcd_dsdf)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.copy(rotated_pc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(pred_rgb.detach().cpu().numpy())\n",
    "    pcd.normals = o3d.utility.Vector3dVector(nrm_dsdf)\n",
    "    rotated_pcds.append(pcd)\n",
    "    \n",
    "    cylinder_segments = line_set_mesh(rotated_box)\n",
    "    # draw 3D bounding boxes around the object\n",
    "    for k in range(len(cylinder_segments)):\n",
    "      rotated_pcds.append(cylinder_segments[k])\n",
    "\n",
    "    # draw 3D coordinate frames around each object\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "    T = abs_pose_outputs[j].camera_T_object\n",
    "    mesh_t = mesh_frame.transform(T)\n",
    "    rotated_pcds.append(mesh_t)\n",
    "    \n",
    "    points_mesh = camera.convert_points_to_homopoints(rotated_pc.T)\n",
    "    points_2d.append(project(_CAMERA.K_matrix, points_mesh).T)\n",
    "    #2D output\n",
    "    points_obb = camera.convert_points_to_homopoints(np.array(rotated_box).T)\n",
    "    box_obb.append(project(_CAMERA.K_matrix, points_obb).T)\n",
    "    xyz_axis = 0.3*np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]).transpose()\n",
    "    sRT = abs_pose_outputs[j].camera_T_object @ abs_pose_outputs[j].scale_matrix\n",
    "    transformed_axes = transform_coordinates_3d(xyz_axis, sRT)\n",
    "    axes.append(calculate_2d_projections(transformed_axes, _CAMERA.K_matrix[:3,:3]))\n",
    "draw_geometries(rotated_pcds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc0fac",
   "metadata": {},
   "source": [
    "## Project 3D Pointclouds and 3D bounding boxes on 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8070cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img = np.copy(img_vis) \n",
    "projected_points_img = show_projected_points(color_img, points_2d)\n",
    "colors_box = [(63, 237, 234)]\n",
    "im = np.array(np.copy(img_vis)).copy()\n",
    "for k in range(len(colors_box)):\n",
    "    for points_2d, axis in zip(box_obb, axes):\n",
    "        points_2d = np.array(points_2d)\n",
    "        im = draw_bboxes(im, points_2d, axis, colors_box[k])\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.axis('off')\n",
    "plt.imshow(im[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f7c872",
   "metadata": {},
   "source": [
    "# Shape Appearance and Pose Optimization\n",
    "\n",
    "Here we run the core optimization loop i.e. update the shape, appearance latent codes as well as absolute poses to fit the single-view test-time RGB-D observation better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b942d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some params for optim and import relevant functions\n",
    "from sdf_latent_codes.get_surface_pointcloud import get_sdfnet\n",
    "from sdf_latent_codes.get_rgb import get_rgbnet\n",
    "from utils.transform_utils import get_abs_pose_vector_from_matrix, get_abs_pose_from_vector\n",
    "from utils.nocs_utils import get_masks_out, get_aligned_masks_segout, get_masked_textured_pointclouds\n",
    "from opt.optimization_all import Optimizer\n",
    "\n",
    "optimization_out = {}\n",
    "latent_opt = []\n",
    "RT_opt = []\n",
    "scale_opt = []\n",
    "\n",
    "do_optim = True    \n",
    "latent_opt = []\n",
    "RT_opt = []\n",
    "scale_opt = []\n",
    "appearance_opt = []\n",
    "colored_opt_pcds = []\n",
    "colored_opt_meshes = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "psi, theta, phi, t = (0, 0, 0, 0)\n",
    "shape_latent_noise = np.random.normal(loc=0, scale=0.02, size=64)\n",
    "add_noise = False\n",
    "viz_type = None\n",
    "\n",
    "# get masks and masked pointclouds of each object in the image\n",
    "depth_ = np.array(depth, dtype=np.float32)*255.0\n",
    "seg_output.convert_to_numpy_from_torch()\n",
    "masks_out = get_masks_out(seg_output, depth_)\n",
    "masks_out = get_aligned_masks_segout(masks_out, output_indices, depth_)\n",
    "masked_pointclouds, areas, masked_rgb = get_masked_textured_pointclouds(masks_out, depth_, left_linear[:,:,::-1], camera = _CAMERA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to draw textured shape with absolute pose after optimization loop\n",
    "def draw_colored_shape(emb, abs_pose, appearance_emb, rgbnet, sdf_latent_code_dir, is_oct_grid= False):\n",
    "    if is_oct_grid:\n",
    "        lod = 7\n",
    "        pcd_dsdf, nrm_dsdf = get_surface_pointclouds_octgrid_viz(emb, lod=lod, sdf_latent_code_dir = sdf_latent_code_dir)\n",
    "    else:\n",
    "        pcd_dsdf = get_surface_pointclouds(emb)\n",
    "\n",
    "    pred_rgb = get_rgb_from_rgbnet(emb, pcd_dsdf, appearance_emb, rgbnet)\n",
    "    #pred_rgb = get_rgb(emb, pcd_dsdf, appearance_emb)\n",
    "\n",
    "    rotated_pc, rotated_box, _ = get_pc_absposes(abs_pose, pcd_dsdf)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.copy(rotated_pc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(pred_rgb.detach().cpu().numpy())\n",
    "    pcd.normals = o3d.utility.Vector3dVector(nrm_dsdf)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536336a9",
   "metadata": {},
   "source": [
    "## Core optimization loop\n",
    "\n",
    "This script will take a couple of minutes to run per image. Note that you can playaround with optimization parameters for best speed/accuracy trade-off i.e. setting a lower LoD or setting number of optimization steps to 100 would suffice in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core optimization loop\n",
    "for k in range(len(shape_emb_outputs)):\n",
    "  print(\"Starting optimization, object:\", k, \"\\n\", \"----------------------------\", \"\\n\")\n",
    "  if viz_type is not None:\n",
    "      optim_foldername = str(output_path) + '/optim_images_'+str(k)\n",
    "      if not os.path.exists(optim_foldername):\n",
    "          os.makedirs(optim_foldername)\n",
    "  else:\n",
    "    optim_foldername = None\n",
    "  \n",
    "  #optimization starts here:\n",
    "  abs_pose = abs_pose_outputs[k]\n",
    "  mask_area = areas[k]\n",
    "  RT, s = get_abs_pose_vector_from_matrix(abs_pose.camera_T_object, abs_pose.scale_matrix, add_noise = False)\n",
    "\n",
    "  if masked_pointclouds[k] is not None:\n",
    "    shape_emb = shape_emb_outputs[k]\n",
    "    appearance_emb = appearance_emb_outputs[k]\n",
    "    decoder = get_sdfnet(sdf_latent_code_dir = sdf_pretrained_dir)\n",
    "    rgbnet = get_rgbnet(rgb_model_dir)\n",
    "    params = {}\n",
    "    weights = {}\n",
    "\n",
    "    if add_noise:\n",
    "      shape_emb += shape_latent_noise\n",
    "\n",
    "    #Set latent vectors/abs pose to optimize here\n",
    "    params['latent'] = shape_emb\n",
    "    params['RT'] = RT\n",
    "    params['scale'] = np.array(s)\n",
    "    params['appearance'] = appearance_emb\n",
    "    weights['3d'] = 1\n",
    "\n",
    "    optimizer = Optimizer(params, rgbnet, device, weights, mask_area)\n",
    "    # Optimize the initial pose estimate\n",
    "    iters_optim = 200\n",
    "    optimizer.optimize_oct_grid(\n",
    "        iters_optim,\n",
    "        masked_pointclouds[k],\n",
    "        masked_rgb[k],\n",
    "        decoder,\n",
    "        rgbnet, \n",
    "        optim_foldername, \n",
    "        viz_type=viz_type\n",
    "    )\n",
    "\n",
    "    #save latent vectors after optimization\n",
    "    latent_opt.append(params['latent'].detach().cpu().numpy())\n",
    "    RT_opt.append(params['RT'].detach().cpu().numpy())\n",
    "    scale_opt.append(params['scale'].detach().cpu().numpy())\n",
    "    appearance_opt.append(params['appearance'].detach().cpu().numpy())\n",
    "    abs_pose = get_abs_pose_from_vector(params['RT'].detach().cpu().numpy(), params['scale'].detach().cpu().numpy())\n",
    "    obj_colored = draw_colored_shape(params['latent'].detach().cpu().numpy(), abs_pose, params['appearance'].detach().cpu().numpy(), rgbnet, sdf_pretrained_dir, is_oct_grid=True)\n",
    "    colored_opt_pcds.append(obj_colored)\n",
    "  else:\n",
    "    latent_opt.append(shape_emb_outputs[k])\n",
    "    RT_opt.append(RT)\n",
    "    scale_opt.append(np.array(s))\n",
    "    appearance_opt.append(appearance_emb_outputs[k])\n",
    "    print(\"Done with optimization, object:\", k, \"\\n\", \"----------------------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d1cef",
   "metadata": {},
   "source": [
    "## Viusalizing optimized 3D output\n",
    "\n",
    "Finally we visualize the optimized 3D shape, appearance and poses. Notice the difference from regressed output specially appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_geometries(colored_opt_pcds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centersnap",
   "language": "python",
   "name": "centersnap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
